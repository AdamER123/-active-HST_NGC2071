{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#purpose is to generate images in the steps below, first collecting some files\n",
    "'''\n",
    "Paths and file needs:\n",
    "*imglams and spitzer_conversions are excel files, right now I have it so you need to put it as same directory as your code (but could later maybe just give it a path to go to - would be smarter)\n",
    "*paths to images and data in general\n",
    "'''\n",
    "\n",
    "#just to check python version - should be 3.7.4\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "#importing libraries\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve, Gaussian2DKernel, Box2DKernel\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "import glob\n",
    "import itertools\n",
    "import matplotlib \n",
    "matplotlib.use('Agg') #invokved b/c just plain matplotlib was insufficient\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #finding the path to every fits images in a directory\n",
    "def im_name_finder(path, file_type):\n",
    "    #Using glob (it's a unix command similar to ls)\n",
    "    #WARNING: using recursive=True...depending how many images you use this could be very slow, it's recommended not to have too many subfolders\n",
    "    #if needed, some example code is commented towards the latter half of this code that could help make an alternative\n",
    "    all_names = glob.glob(path, recursive=True)\n",
    "\n",
    "    #IMPORTANT: Using \"fit\" here because it is inclusive of both fits and FIT...some files end in \"FIT\" and need to be included\n",
    "    #using s.lower() include uppercase names\n",
    "    im_names = [s for s in all_names if 'fit' in s.lower()]\n",
    "\n",
    "    return im_names\n",
    "\n",
    "\n",
    "'''now convolve my image with a PSF of the image we're projecting ONTO\n",
    "an approx PSF can be found by assuming a 2D Gaussian func with a width (a FWHM) of the diffrac limit\n",
    "that is the st dev of the Gaussian is about the st dev is about = lambda/D\n",
    "a list of PSFs are found on https://docs.astropy.org/en/stable/convolution/kernels.html\n",
    "\n",
    "Notes:\n",
    "FIRST: always must convert hdu1_pixtorad to radians! It's inconsistent otherwise, and lambda/D is generally in radians\n",
    "\n",
    "what we're using for the gaussian width is the FWHM, not the radius of the first ring of the diffraction pattern,\n",
    "so it's 1.2 not 1.22 times lambda/D\n",
    "\n",
    "D is 85 cm for spitzer\n",
    "D is 2.4 m for hubble\n",
    "'''\n",
    "\n",
    "def im_conv(D, hdu_pix_torad, hdu_dat, lam, kern):\n",
    "    #gaussian kernel\n",
    "    if kern == 'gauss':\n",
    "        #update: usually cannot find wavelength but these headers are well-labeled    \n",
    "        #finding angular resolution...the FWHM of our Gaussian PSF\n",
    "        res = 1.2 * lam / D         #resolution in radians\n",
    "        res = res / hdu_pix_torad        #so converting to pixels\n",
    "\n",
    "        #finding PSF and then calculating the convolution of our image and the PSF of the image we're projecting onto\n",
    "        kernel = Gaussian2DKernel(res)\n",
    "\n",
    "    #box kernel\n",
    "    if kern == 'box':\n",
    "        kernel = Box2DKernel(16.)\n",
    "\n",
    "    hdu_conv = convolve(hdu_dat, kernel)\n",
    "    return hdu_conv\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "#setting up a new fits file to be saved and viewed in DS9\n",
    "#primarily to save the image we reprojected, but can also be used to save the convolved images\n",
    "def fits_saver(array, wcs_header, name, save_path):\n",
    "    '''\n",
    "    array is a 2d array of data - could be from reprojecting one image onto another or from convolution\n",
    "    wcs_header is a header containing the wcs coords of the image that we projected onto or of the orig image (if from the convolution)\n",
    "    name is the path to some image you're using. It will get string split at the / character, and the func only takes the last element of that splitting\n",
    "    save_path is the folder you want to save to...recommended to also add something to the start of the images names to make it clear what you did to them (e.g. 'Regridded/regrid_')\n",
    "    '''\n",
    "\n",
    "    #creating a new file and adding the reprojected array of data as well as the WCS that we projected onto\n",
    "    hdu_new = fits.PrimaryHDU(array, header=wcs_header)\n",
    "    hdul = fits.HDUList([hdu_new])\n",
    "    \n",
    "    #saving the file\n",
    "    if name.find('FIT') == -1: #needed if file end incorrect\n",
    "        new_filename = name.split('/')[-1]  #grabs the file name we were using from before\n",
    "        hdul.writeto(save_path+new_filename, overwrite=True)\n",
    "    else:\n",
    "        name_fixfit = name[:-3] + 'fits'\n",
    "        new_filename = name_fixfit.split('/')[-1]  #grabs the file name we were using from before\n",
    "        hdul.writeto(save_path+new_filename, overwrite=True)\n",
    "        \n",
    "    return (save_path+new_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses cross correlation\n",
    "Background paper examples\n",
    "https://articles.adsabs.harvard.edu/pdf/1992ApJ...392..145R or https://iopscience.iop.org/article/10.1086/517493/pdf\n",
    "\n",
    "Example stack overflow references...\n",
    "https://stackoverflow.com/questions/24768222/how-to-detect-a-shift-between-images (the best)\n",
    "https://stackoverflow.com/questions/61114057/how-to-estimate-motion-with-ftt-and-cross-correlation (not bad)\n",
    "https://stackoverflow.com/questions/189943/how-can-i-quantify-difference-between-two-images (good conceptual explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we try to grab a 1.64 mic fits image with minimal \n",
    "\n",
    "path = '../continuum_subtract/nocont*164*.fits' # #using ** will grab all files even in subdirectories WARNING takes longer\n",
    "im_names_n2071 = sorted(im_name_finder(path, 'fit')) #im_finder is basically glob.glob\n",
    "im_names_n2071 = [i.replace('\\\\', '/') for i in im_names_n2071]\n",
    "print(im_names_n2071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list = [fits.open(i) for i in im_names_n2071]\n",
    "\n",
    "#initializing some lists to be used\n",
    "hdu_data_list = []\n",
    "hdu_header_list = []\n",
    "\n",
    "count = 0\n",
    "for hdu_data in hdu_list:   \n",
    "    #reading in data for general use  and header for wcs\n",
    "    #converting by times by flam * bw from e-/sec...should get units of erg/cm^2/sec as above\n",
    "    \n",
    "    #needed because the second image in this list is negative...\n",
    "    if count == 1:\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "    hdu_data_list.append(sign * hdu_data[0].data) # * hdu_list[0].header['PHOTFLAM'] * hdu_list[0].header['PHOTBW'])\n",
    "    hdu_header_list.append(hdu_data[0].header)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our plotting function\n",
    "def implot(data, w, wcscond, vmax_p):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    if  wcscond == True:\n",
    "        fig.add_subplot(111, projection=w)\n",
    "    else:\n",
    "        fig.add_subplot(111)\n",
    "    \n",
    "    #for christmas turn on GnRd\n",
    "    #plt.cm.get_cmap('Blues', 6) is another option\n",
    "    #can also use RdBu...\n",
    "    #otherwise just use plt.cm.viridis b/c it works\n",
    "    plt.imshow(data, origin='lower', cmap=plt.cm.viridis, vmin =0, vmax=vmax_p)\n",
    "    plt.xlabel('RA')\n",
    "    plt.ylabel('Dec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in a region file + indexing for cutouts\n",
    "Seems like doing it with cutout2d for once worked better than doing it by hand...so let's just go with it.\n",
    "\n",
    "Will help to then overlay this on the difference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trying this with cutout2d\n",
    "# https://docs.astropy.org/en/stable/nddata/utils.html#cutout-images\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy import units as u\n",
    "from astropy.wcs.utils import skycoord_to_pixel\n",
    "from astropy.coordinates import SkyCoord, FK5, ICRS\n",
    "from photutils.aperture import EllipticalAperture\n",
    "\n",
    "#trying it by hand\n",
    "\n",
    "#known pixel size\n",
    "hst_pixsize = 0.12825 #arcsec\n",
    "\n",
    "# f = open('moving_blobs.reg', 'r')\n",
    "# f1 = open('precession_inflections.reg', 'r')\n",
    "f1 = open('epoch2_361c_byhand_ellipses.reg', 'r')\n",
    "file1_output = []\n",
    "\n",
    "#looping through file \n",
    "for line in f1:\n",
    "    file1_output.append(line)\n",
    "f1.close()\n",
    "\n",
    "#fix file contents\n",
    "#contents are ra, dec, width, height, rotation\n",
    "file1_output = [i[8:-2] for i in file1_output[3:]]\n",
    "\n",
    "#next step is for image, loop through all regions and make a list of region properties\n",
    "ra_pix_list = []\n",
    "dec_pix_list = []\n",
    "rad_a_list = []\n",
    "rad_b_list = []\n",
    "rotation_list = []\n",
    "\n",
    "#coordinate details\n",
    "split_params1 = [i.split(',') for i in file1_output[:-1]]\n",
    "ra_hms_list = [ra1[0].split(':')[0]+'h' + ra1[0].split(':')[1]+'m' + ra1[0].split(':')[2]+'s' for ra1 in split_params1]\n",
    "dec_dms_list = [dec1[1].split(':')[0]+'d' + dec1[1].split(':')[1]+'m' + dec1[1].split(':')[2]+'s' for dec1 in split_params1]\n",
    "pos_list = [SkyCoord(ra+' '+dec, frame=FK5, unit=(u.hourangle, u.deg)) for ra, dec in zip(ra_hms_list, dec_dms_list)]\n",
    "\n",
    "#this isn't working for some reason...skycoord_to_pixel doesn't seem to like units? not sure\n",
    "ra_pix_list = [skycoord_to_pixel(i, wcs=WCS(hdu_header_list[0]))[0] for i in pos_list]\n",
    "dec_pix_list = [skycoord_to_pixel(i, wcs=WCS(hdu_header_list[0]))[1] for i in pos_list]\n",
    "\n",
    "#aperture size details\n",
    "rad_a_list = [[1./hst_pixsize * float(rad_a1[2][:-1])] for rad_a1 in split_params1]\n",
    "rad_b_list = [[1./hst_pixsize * float(rad_b1[2][:-1])] for rad_b1 in split_params1]\n",
    "rotation_list = [[float(rot1[4])] for rot1 in split_params1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting markers on image for reference\n",
    "\n",
    "Ideally, we should see something like a zig-zig or a parallelogram...\n",
    "\n",
    "Also check order of points we ended up with\n",
    "\n",
    "Looks like the source star (361C) is labeled as ind = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hdu_data_list[0], vmin=0, vmax=1e-17)\n",
    "\n",
    "# plt.scatter(ra_pix_list[3], dec_pix_list[3], color='red') \n",
    "plt.scatter(ra_pix_list, dec_pix_list, color='red') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring wavelength and opening angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wavelength\n",
    "#separation's .value method outputs value in degrees, have to convert to radians...\n",
    "print(pos_list[0].separation(pos_list[2]))\n",
    "print(pos_list[1].separation(pos_list[3]))\n",
    "\n",
    "lam_helix = np.pi / 180. * np.mean([pos_list[0].separation(pos_list[2]).value, pos_list[1].separation(pos_list[3]).value])\n",
    "print(lam_helix)\n",
    "\n",
    "#opening angle, here we care about vertex ind = 3 and two points ind=0 , 1 \n",
    "#then we apply dot product...\n",
    "a = (ra_pix_list[3] - ra_pix_list[0], dec_pix_list[3] - dec_pix_list[0])\n",
    "b = (ra_pix_list[3] - ra_pix_list[1], dec_pix_list[3] - dec_pix_list[1])\n",
    "\n",
    "full_opening_rad = np.arccos(np.dot(a, b) / (np.sqrt(np.dot(a,a)) * np.sqrt(np.dot(b,b))))\n",
    "\n",
    "print(full_opening_rad / 2. * 180/2/np.pi) #to check, printing a half opening angle in degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving precessional parameters from binary system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://iopscience.iop.org/article/10.3847/1538-4357/ac7464/pdf\n",
    "#this gives \n",
    "semi_major = 40/2. #au or 0.2\" according to Trinidad 09 ~ 80 au ?\n",
    "m_pri = 1.475 #msun\n",
    "ma_mb = np.mean([1.3, 2.5]) #secondary/primary, according to cheng et al 2022 (link above), given as m_pri / m_sec\n",
    "m_sec = m_pri / ma_mb\n",
    "m_sum = m_pri + m_sec\n",
    "mu = m_pri / m_sum\n",
    "tau_orbit = np.sqrt(semi_major**3 / m_sum) #years\n",
    "\n",
    "'''\n",
    "tau_prec / np.cos(full_opening_rad/2) = 15/32 * mu/np.sqrt(1-mu) * sigma**1.5 / tau_orbit\n",
    "\n",
    "tau_prec = np.sqrt( lam_helix * dist / (vtan * np.cos(full_opening_rad/2)) )\n",
    "\n",
    "np.sqrt( lam_helix * dist / (vtan * np.cos(full_opening_rad/2)) ) \n",
    "    = 15/32 * mu/np.sqrt(1-mu) * sigma**1.5 / tau_orbit * np.cos(full_opening_rad/2) \n",
    "    \n",
    "lam_helix * dist / (vtan * np.cos(full_opening_rad/2))  \n",
    "    = (15/32 * mu/np.sqrt(1-mu) * sigma**1.5 / tau_orbit * np.cos(full_opening_rad/2) )**2\n",
    "    \n",
    "lam_helix / np.cos(full_opening_rad/2)**3\n",
    "    = (15/32 * mu/np.sqrt(1-mu) * sigma**1.5 / tau_orbit )**2 * vtan / dist\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pri, m_sec, m_sum, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 1.327e19 #430 pc -> m\n",
    "vtan = 400 * 1000 #km/s -> m/s\n",
    "sigma = 1/3\n",
    "\n",
    "full_opening_rad = 20 * np.pi / 180 #degrees to rad\n",
    "\n",
    "lam_helix = (15/32 * mu/np.sqrt(1-mu) * sigma**1.5 / tau_orbit/3.154e7 )**2 * vtan / dist * np.cos(full_opening_rad/2)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_helix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the sinusoidal formula from Anglada 07 Sec 4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting resulting image\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from astropy.utils.data import get_pkg_data_filename\n",
    "from astropy.visualization import ZScaleInterval, ImageNormalize\n",
    "from astropy.visualization.stretch import SinhStretch, LogStretch\n",
    "from astropy.wcs import WCS\n",
    "from astropy.wcs.utils import skycoord_to_pixel\n",
    "import matplotlib.ticker\n",
    "from matplotlib.ticker import LogFormatter, LogLocator, FixedLocator, FixedFormatter\n",
    "                \n",
    "#minor formatting for ticks\n",
    "# plt.rcParams['xtick.labeltop'] = plt.rcParams['xtick.labelright'] = False\n",
    "plt.rcParams['xtick.major.size'] = 14\n",
    "plt.rcParams['ytick.major.size'] = 14\n",
    "\n",
    "#best fit to a single sine curve\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "wcs = WCS(hdu_header_list[0])\n",
    "ax = plt.subplot(projection=wcs)\n",
    "\n",
    "#plotting data\n",
    "plt.scatter(ra_pix_list, dec_pix_list, color='black') \n",
    "\n",
    "#treat x as the vertical units and y as the horizontal units in current image config...\n",
    "# y_arr = np.linspace(-750, 750, 50)\n",
    "# y0 = 1500 # lam_helix * np.pi * 2 # ra_pix_list[3]\n",
    "# x0 = dec_pix_list[0]+50\n",
    "# x = -y_arr * np.tan(full_opening_rad/2) * np.cos(2.*np.pi/lam_helix * (np.abs(y_arr) - y0))\n",
    "# # x = y_arr*2 * np.tan(15*np.pi/180/2) * np.cos(2*np.pi*1.5 * (np.abs(y_arr) - int(y0)))\n",
    "# # x = (y_arr+lam_helix / (2.*np.pi)) * np.tan(full_opening_rad / 2.) * np.cos(2.*np.pi/lam_helix * (np.abs(y_arr) - int(y0)))\n",
    "# ax.plot(y_arr+y0, x+x0, color='red')\n",
    "\n",
    "#plotting best fit \n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def y_precess(x, amp_prec, amp_orb, lam_prec, lam_orb, x0, y0):\n",
    "        return amp_prec * np.sin(2*np.pi / lam_prec * (np.abs(x) - x0)) + \\\n",
    "                amp_orb * np.sin(2*np.pi / lam_orb * (np.abs(x) - x0)) + y0 \n",
    "\n",
    "guess_amp_prec, guess_amp_orb, guess_lam_prec, guess_lam_orb, guess_x0, guess_y0 = \\\n",
    "    100, 10, 1000, 50, 1000, 1000\n",
    "guess = [guess_amp_prec, guess_amp_orb, guess_lam_prec, guess_lam_orb, guess_x0, guess_y0]\n",
    "params, cov = curve_fit(y_precess, ra_pix_list, dec_pix_list, p0=guess)\n",
    "amp_prec, amp_orb, lam_prec, lam_orb, x0, y0 = params\n",
    "\n",
    "dist_interp = np.linspace(np.min(ra_pix_list), np.max(ra_pix_list), 500) #plotting\n",
    "ax.plot(dist_interp, y_precess(dist_interp, amp_prec, amp_orb, lam_prec, lam_orb, x0, y0), 'k--')\n",
    "\n",
    "#plotting image\n",
    "norm = ImageNormalize(stretch=LogStretch(), vmin=1e-18, vmax=100e-18)#vmin=data_interval[0], vmax=data_interval[1])\n",
    "im = ax.imshow(hdu_data_list[0], norm=norm, cmap='tab20b') # plotting image for reference\n",
    "\n",
    "#general formatting\n",
    "ax.coords.grid(True, color='white', ls='solid', linewidth=0.75) #adding gridlines\n",
    "ax.coords[0].set_axislabel('Right Ascension (J2000)', fontsize=30)\n",
    "ax.coords[1].set_axislabel('Declination (J2000)', fontsize=30)   \n",
    "ax.tick_params(axis='x', labelbottom=True, labeltop=False, labelright=False)\n",
    "ax.tick_params(axis='x', labelsize=20)\n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "ax.invert_yaxis() #done because it's nicer, with declination increasing\n",
    "ax.invert_xaxis() #done because it's nicer with ra decreasing\n",
    "\n",
    "#colorbar, see 3rd answer from https://stackoverflow.com/questions/18195758/set-matplotlib-colorbar-size-to-match-graph\n",
    "cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "cbar = plt.colorbar(im, cax=cax) \n",
    "\n",
    "#color bar label and tick labels\n",
    "sub_labels = [0.1, 0.25, 10]\n",
    "cbar.locator = LogLocator(base=10, subs=sub_labels)\n",
    "cbar.ax.tick_params(labelsize=20)\n",
    "cbar.update_ticks()\n",
    "cbar.ax.yaxis.get_offset_text().set_fontsize(0)\n",
    "cbar.set_label(label=r'$\\rm Intensity~(x~{10}^{-18}~erg/s/{cm}^{2}/pix)$', size=20)\n",
    "# cbar_tickfont = [cbar.ax.set_yticklabels(labels=cbar.ax.get_yticklabels())[i].set_fontweight('normal') \\\n",
    "#                      for i in range(len(cbar.ax.get_yticklabels()))]\n",
    "\n",
    "plt.savefig('hops_361c_precession.png', dpi=300)\n",
    "plt.savefig('hops_361c_precession.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pc, pc, pix\n",
    "#amp_prec, amp_orb, lam_prec, lam_orb, x0, y0\n",
    "print(amp_prec, lam_prec*hst_pixsize, lam_prec*hst_pixsize*430/206265)\n",
    "\n",
    "print(amp_orb, lam_orb*hst_pixsize, lam_orb*hst_pixsize*430/206265)\n",
    "\n",
    "print(x0, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatively, use fit params to extract binary params...\n",
    "\n",
    "From this, we can get the precession period given v_jet\n",
    "\n",
    "Then we need to get sigma and the mass ratio (and/or compare with the literature) under assumptions\n",
    "(close binary, distant blob of mass, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 1.327e19 #430 pc -> m\n",
    "vtan = 400 * 1000 #km/s -> m/s\n",
    "\n",
    "tau_prec = np.sqrt( lam_helix * dist / (vtan * np.cos(full_opening_rad/2)) )\n",
    "\n",
    "#sigma is rd / a, can perhaps use 1/3\n",
    "sigma = 1./2.\n",
    "\n",
    "#orbital period and/or orbital radius are then found as a function of mu, which we technically know\n",
    "#https://iopscience.iop.org/article/10.3847/1538-4357/ac7464/pdf\n",
    "#can alternatively use Trinidad 2009\n",
    "# mu = ... np.mean([1.3, 2.5]) #secondary/primary, according to cheng et al 2022 (link above), given as mpri/m_sum\n",
    "tau_orbit = tau_prec * 15/32 * mu/np.sqrt(1-mu) * sigma**1.5 *np.cos(full_opening_rad/2)\n",
    "\n",
    "print(tau_prec, 'seconds?', mu, 'mpri/msum', tau_orbit, 'seconds?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
