{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891f067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arubi\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#just to check python version - should be 3.7.4\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "#importing libraries\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve, Gaussian2DKernel, Box2DKernel\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "import glob\n",
    "import itertools\n",
    "import matplotlib \n",
    "matplotlib.use('Agg') #invokved b/c just plain matplotlib was insufficient\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30177e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #finding the path to every fits images in a directory\n",
    "def im_name_finder(path, file_type):\n",
    "    #Using glob (it's a unix command similar to ls)\n",
    "    #WARNING: using recursive=True...depending how many images you use this could be very slow, it's recommended not to have too many subfolders\n",
    "    #if needed, some example code is commented towards the latter half of this code that could help make an alternative\n",
    "    all_names = glob.glob(path, recursive=True)\n",
    "\n",
    "    #IMPORTANT: Using \"fit\" here because it is inclusive of both fits and FIT...some files end in \"FIT\" and need to be included\n",
    "    #using s.lower() include uppercase names\n",
    "    im_names = [s for s in all_names if 'fit' in s.lower()]\n",
    "\n",
    "    return im_names\n",
    "\n",
    "\n",
    "'''now convolve my image with a PSF of the image we're projecting ONTO\n",
    "an approx PSF can be found by assuming a 2D Gaussian func with a width (a FWHM) of the diffrac limit\n",
    "that is the st dev of the Gaussian is about the st dev is about = lambda/D\n",
    "a list of PSFs are found on https://docs.astropy.org/en/stable/convolution/kernels.html\n",
    "\n",
    "Notes:\n",
    "FIRST: always must convert hdu1_pixtorad to radians! It's inconsistent otherwise, and lambda/D is generally in radians\n",
    "\n",
    "what we're using for the gaussian width is the FWHM, not the radius of the first ring of the diffraction pattern,\n",
    "so it's 1.2 not 1.22 times lambda/D\n",
    "\n",
    "D is 85 cm for spitzer\n",
    "D is 2.4 m for hubble\n",
    "'''\n",
    "\n",
    "def im_conv(D, hdu_pix_torad, hdu_dat, lam, kern):\n",
    "    #gaussian kernel\n",
    "    if kern == 'gauss':\n",
    "        #update: usually cannot find wavelength but these headers are well-labeled    \n",
    "        #finding angular resolution...the FWHM of our Gaussian PSF\n",
    "        res = 1.2 * lam / D         #resolution in radians\n",
    "        res = res / hdu_pix_torad        #so converting to pixels\n",
    "\n",
    "        #finding PSF and then calculating the convolution of our image and the PSF of the image we're projecting onto\n",
    "        kernel = Gaussian2DKernel(res)\n",
    "\n",
    "    #box kernel\n",
    "    if kern == 'box':\n",
    "        kernel = Box2DKernel(16.)\n",
    "\n",
    "    hdu_conv = convolve(hdu_dat, kernel)\n",
    "    return hdu_conv\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "#setting up a new fits file to be saved and viewed in DS9\n",
    "#primarily to save the image we reprojected, but can also be used to save the convolved images\n",
    "def fits_saver(array, wcs_header, name, save_path):\n",
    "    '''\n",
    "    array is a 2d array of data - could be from reprojecting one image onto another or from convolution\n",
    "    wcs_header is a header containing the wcs coords of the image that we projected onto or of the orig image (if from the convolution)\n",
    "    name is the path to some image you're using. It will get string split at the / character, and the func only takes the last element of that splitting\n",
    "    save_path is the folder you want to save to...recommended to also add something to the start of the images names to make it clear what you did to them (e.g. 'Regridded/regrid_')\n",
    "    '''\n",
    "\n",
    "    #creating a new file and adding the reprojected array of data as well as the WCS that we projected onto\n",
    "    hdu_new = fits.PrimaryHDU(array, header=wcs_header)\n",
    "    hdul = fits.HDUList([hdu_new])\n",
    "    \n",
    "    #saving the file\n",
    "    if name.find('FIT') == -1: #needed if file end incorrect\n",
    "        new_filename = name.split('/')[-1]  #grabs the file name we were using from before\n",
    "        hdul.writeto(save_path+new_filename, overwrite=True)\n",
    "    else:\n",
    "        name_fixfit = name[:-3] + 'fits'\n",
    "        new_filename = name_fixfit.split('/')[-1]  #grabs the file name we were using from before\n",
    "        hdul.writeto(save_path+new_filename, overwrite=True)\n",
    "        \n",
    "    return (save_path+new_filename)\n",
    "\n",
    "#our plotting function\n",
    "def implot(data, w, wcscond, vmax_p):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    if  wcscond == True:\n",
    "        fig.add_subplot(111, projection=w)\n",
    "    else:\n",
    "        fig.add_subplot(111)\n",
    "    \n",
    "    #for christmas turn on GnRd\n",
    "    #plt.cm.get_cmap('Blues', 6) is another option\n",
    "    #can also use RdBu...\n",
    "    #otherwise just use plt.cm.viridis b/c it works\n",
    "    plt.imshow(data, origin='lower', cmap=plt.cm.viridis, vmin =0, vmax=vmax_p)\n",
    "    plt.xlabel('RA')\n",
    "    plt.ylabel('Dec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d800d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../scaling_for_motions/160_epoch1.fits', '../scaling_for_motions/160_epoch1_scaled.fits', '../scaling_for_motions/160_epoch2_synth.fits', '../scaling_for_motions/160_epoch2_synth_scaled.fits']\n",
      "['../scaling_for_motions/160_epoch1.fits', '../scaling_for_motions/160_epoch2_synth.fits']\n"
     ]
    }
   ],
   "source": [
    "path = '../scaling_for_motions/160_epoch*.fits' # #using ** will grab all files even in subdirectories WARNING takes longer\n",
    "im_names_n2071 = sorted(im_name_finder(path, 'fit')) #im_finder is basically glob.glob\n",
    "im_names_n2071 = [i.replace('\\\\', '/') for i in im_names_n2071]\n",
    "print(im_names_n2071)\n",
    "\n",
    "im_names_n2071 = [im_names_n2071[0], im_names_n2071[2]]\n",
    "print(im_names_n2071)\n",
    "\n",
    "hdu_list = [fits.open(i) for i in im_names_n2071]\n",
    "\n",
    "#initializing some lists to be used\n",
    "hdu_data_list = []\n",
    "hdu_header_list = []\n",
    "\n",
    "count = 0\n",
    "for hdu_data in hdu_list:   \n",
    "    #reading in data for general use  and header for wcs\n",
    "    #converting by times by flam * bw from e-/sec...should get units of erg/cm^2/sec as above\n",
    "    \n",
    "    #needed because the second image in this list is negative...\n",
    "    if count == 1:\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "    hdu_data_list.append(sign * hdu_data[0].data) # * hdu_list[0].header['PHOTFLAM'] * hdu_list[0].header['PHOTBW'])\n",
    "    hdu_header_list.append(hdu_data[0].header)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fba18",
   "metadata": {},
   "source": [
    "# tobac\n",
    "\n",
    "from https://github.com/tobac-project/tobac/tree/main/tobac\n",
    "\n",
    "\"tobac v1.0: towards a flexible framework for tracking and analysis of clouds in diverse datasets\"\n",
    "by Heikenfeld et al 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2e00a32b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Intensity of seed image must be greater than that of the mask image for reconstruction by erosion.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [122]\u001b[0m, in \u001b[0;36m<cell line: 101>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m seed[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-16\u001b[39m \u001b[38;5;66;03m# image.max()\u001b[39;00m\n\u001b[0;32m    100\u001b[0m mask \u001b[38;5;241m=\u001b[39m image\n\u001b[1;32m--> 101\u001b[0m filled \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merosion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(image)\n\u001b[0;32m    104\u001b[0m seed[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mmin()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\_shared\\utils.py:282\u001b[0m, in \u001b[0;36mdeprecate_kwarg.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m         kwargs[new_arg] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old_arg)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Call the function with the fixed arguments\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\morphology\\grayreconstruct.py:134\u001b[0m, in \u001b[0;36mreconstruction\u001b[1;34m(seed, mask, method, footprint, offset)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntensity of seed image must be less than that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the mask image for reconstruction by dilation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merosion\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(seed \u001b[38;5;241m<\u001b[39m mask):\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntensity of seed image must be greater than that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the mask image for reconstruction by erosion.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_grayreconstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reconstruction_loop\n",
      "\u001b[1;31mValueError\u001b[0m: Intensity of seed image must be greater than that of the mask image for reconstruction by erosion."
     ]
    }
   ],
   "source": [
    "# import iris\n",
    "# iris.load_cube(hdu_data_list)\n",
    "\n",
    "#instead we should base this off of their functions directly...\n",
    "#based on https://github.com/tobac-project/tobac/blob/main/tobac/analysis.py\n",
    "def calculate_nearestneighbordistance(features, method_distance=None):\n",
    "    from itertools import combinations\n",
    "\n",
    "    features[\"min_distance\"] = np.nan\n",
    "    for time_i, features_i in features.groupby(\"time\"):\n",
    "        logging.debug(str(time_i))\n",
    "        indeces = combinations(features_i.index.values, 2)\n",
    "        # Loop over combinations to remove features that are closer together than min_distance and keep larger one (either higher threshold or larger area)\n",
    "        distances = []\n",
    "        for index_1, index_2 in indeces:\n",
    "            if index_1 is not index_2:\n",
    "                distance = calculate_distance(\n",
    "                    features_i.loc[index_1],\n",
    "                    features_i.loc[index_2],\n",
    "                    method_distance=method_distance,\n",
    "                )\n",
    "                distances.append(\n",
    "                    pd.DataFrame(\n",
    "                        {\"index_1\": index_1, \"index_2\": index_2, \"distance\": distance},\n",
    "                        index=[0],\n",
    "                    )\n",
    "                )\n",
    "        if any([x is not None for x in distances]):\n",
    "            distances = pd.concat(distances, ignore_index=True)\n",
    "            for i in features_i.index:\n",
    "                min_distance = distances.loc[\n",
    "                    (distances[\"index_1\"] == i) | (distances[\"index_2\"] == i),\n",
    "                    \"distance\",\n",
    "                ].min()\n",
    "                features.at[i, \"min_distance\"] = min_distance\n",
    "    return features\n",
    "\n",
    "def calculate_overlap(\n",
    "    track_1, track_2, min_sum_inv_distance=None, min_mean_inv_distance=None\n",
    "):\n",
    "    cells_1 = track_1[\"cell\"].unique()\n",
    "    #    n_cells_1_tot=len(cells_1)\n",
    "    cells_2 = track_2[\"cell\"].unique()\n",
    "    overlap = pd.DataFrame()\n",
    "    for i_cell_1, cell_1 in enumerate(cells_1):\n",
    "        for cell_2 in cells_2:\n",
    "            track_1_i = track_1[track_1[\"cell\"] == cell_1]\n",
    "            track_2_i = track_2[track_2[\"cell\"] == cell_2]\n",
    "            track_1_i = track_1_i[track_1_i[\"time\"].isin(track_2_i[\"time\"])]\n",
    "            track_2_i = track_2_i[track_2_i[\"time\"].isin(track_1_i[\"time\"])]\n",
    "            if not track_1_i.empty:\n",
    "                n_overlap = len(track_1_i)\n",
    "                distances = []\n",
    "                for i in range(len(track_1_i)):\n",
    "                    distance = calculate_distance(\n",
    "                        track_1_i.iloc[[i]], track_2_i.iloc[[i]], method_distance=\"xy\"\n",
    "                    )\n",
    "                    distances.append(distance)\n",
    "                #                mean_distance=np.mean(distances)\n",
    "                mean_inv_distance = np.mean(1 / (1 + np.array(distances) / 1000))\n",
    "                #                mean_inv_squaredistance=np.mean(1/(1+(np.array(distances)/1000)**2))\n",
    "                sum_inv_distance = np.sum(1 / (1 + np.array(distances) / 1000))\n",
    "                #                sum_inv_squaredistance=np.sum(1/(1+(np.array(distances)/1000)**2))\n",
    "                overlap = overlap.append(\n",
    "                    {\n",
    "                        \"cell_1\": cell_1,\n",
    "                        \"cell_2\": cell_2,\n",
    "                        \"n_overlap\": n_overlap,\n",
    "                        #                                'mean_distance':mean_distance,\n",
    "                        \"mean_inv_distance\": mean_inv_distance,\n",
    "                        #                                'mean_inv_squaredistance':mean_inv_squaredistance,\n",
    "                        \"sum_inv_distance\": sum_inv_distance,\n",
    "                        #                                'sum_inv_squaredistance':sum_inv_squaredistance\n",
    "                    },\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "    if min_sum_inv_distance:\n",
    "        overlap = overlap[(overlap[\"sum_inv_distance\"] >= min_sum_inv_distance)]\n",
    "    if min_mean_inv_distance:\n",
    "        overlap = overlap[(overlap[\"mean_inv_distance\"] >= min_mean_inv_distance)]\n",
    "\n",
    "    return overlap\n",
    "\n",
    "\n",
    "'''\n",
    "otherwise, the methods use:\n",
    "-watershed segmentation\n",
    "-trackpy\n",
    "-sci-kit image's morphology.reconstruction, either splitting image around dilation (max) or erosion (min)\n",
    "...reconstruction might not be a bad idea...nevermind, don't do it...\n",
    "'''\n",
    "\n",
    "# from skimage.morphology import reconstruction\n",
    "# image = hdu_data_list[0]\n",
    "\n",
    "# seed = np.copy(image)\n",
    "# seed[1:-1, 1:-1] = 1e-16 # image.max()\n",
    "# mask = image\n",
    "# filled = reconstruction(seed, mask, method='erosion')\n",
    "\n",
    "# seed = np.copy(image)\n",
    "# seed[1:-1, 1:-1] = image.min()\n",
    "# rec = reconstruction(seed, mask, method='dilation')\n",
    "\n",
    "\n",
    "# #their main goal is thresholding, see: https://github.com/tobac-project/tobac/blob/main/tobac/feature_detection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e82481",
   "metadata": {},
   "source": [
    "*Seems difficult to use because of necessity of IRIS and data cubes\n",
    "\n",
    "# Testing TINT = TINT is not TITAN\n",
    "\n",
    "See https://github.com/openradar/TINT/tree/master/tint \\\n",
    "\"An Adaptive Tracking Algorithm for Convection in Simulated and Remote Sensing Data\" by Bhupendra et al 2021\n",
    "\n",
    "Here the data set also doesn't work well for us, but we can perhaps either base this off of TITAN or use their functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d328a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/openradar/TINT/blob/master/tint/phase_correlation.py\n",
    "    \n",
    "https://github.com/openradar/TINT/blob/master/tint/matching.py\n",
    "        \n",
    "https://github.com/tobac-project/tobac/blob/main/tobac/feature_detection.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
